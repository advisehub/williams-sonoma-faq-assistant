import { generateText } from "ai"
import { openai } from "@ai-sdk/openai"
import { findRelevantFAQs } from "./vector-search"

// Define evaluation types
type EvaluationQuestion = {
  question: string
  expectedTopics: string[]
}

type EvaluationResult = {
  question: string
  answer: string
  relevantFAQs: Array<{
    question: string
    answer: string
    similarity: number
  }>
  evaluation: {
    factualAccuracy: number // 0-10
    relevance: number // 0-10
    completeness: number // 0-10
    staysWithinFAQScope: number // 0-10
    overallScore: number // 0-10
    feedback: string
  }
}

// Sample evaluation questions
const evaluationQuestions: EvaluationQuestion[] = [
  {
    question: "How do I track my order?",
    expectedTopics: ["order tracking", "shipping status"],
  },
  {
    question: "What is your return policy?",
    expectedTopics: ["returns", "refunds", "policy"],
  },
  {
    question: "Do you offer international shipping?",
    expectedTopics: ["international", "shipping", "delivery"],
  },
  {
    question: "How do I redeem a gift card?",
    expectedTopics: ["gift card", "redeem", "payment"],
  },
  {
    question: "Can I change or cancel my order?",
    expectedTopics: ["cancel", "change", "modify", "order"],
  },
]

// Function to evaluate the assistant's performance
export async function evaluateAssistant(): Promise<EvaluationResult[]> {
  const results: EvaluationResult[] = []

  for (const evalQuestion of evaluationQuestions) {
    // Get relevant FAQs for the question
    const relevantFAQs = await findRelevantFAQs(evalQuestion.question)

    // Create context from the relevant FAQs
    const faqContext =
      relevantFAQs.length > 0
        ? relevantFAQs.map((faq) => `Question: ${faq.question}\nAnswer: ${faq.answer}`).join("\n\n")
        : "No relevant FAQ information found."

    // Generate answer using the same system as the assistant
    const { text: answer } = await generateText({
      model: openai("gpt-4o"),
      system: `You are a helpful assistant for Williams-Sonoma customer support. 
      Answer ONLY based on the Williams-Sonoma FAQ information provided below.
      If the information needed to answer the question is not in the FAQ context, say "I don't have that specific information in the Williams-Sonoma FAQ. You may want to contact customer service directly for more details."
      Do not make up information or use knowledge outside of the provided FAQ context.
      
      FAQ CONTEXT:
      ${faqContext}`,
      prompt: evalQuestion.question,
    })

    // Evaluate the answer
    const { text: evaluationText } = await generateText({
      model: openai("gpt-4o"),
      system: `You are an expert evaluator of AI assistant responses. 
      You will be given a question, the expected topics that should be covered, and an answer generated by an AI assistant.
      The assistant is supposed to ONLY use information from Williams-Sonoma's FAQ and not make up information.
      
      Evaluate the answer on the following criteria on a scale of 0-10:
      1. Factual Accuracy: Does the answer contain correct information based on the provided FAQs?
      2. Relevance: How relevant is the answer to the question asked?
      3. Completeness: Does the answer cover all aspects of the question?
      4. Stays Within FAQ Scope: Does the answer only use information from the FAQs and not make up details?
      5. Overall Score: An overall assessment of the answer quality.
      
      Also provide brief feedback explaining your ratings.
      
      Format your response as a JSON object with the following structure:
      {
        "factualAccuracy": number,
        "relevance": number,
        "completeness": number,
        "staysWithinFAQScope": number,
        "overallScore": number,
        "feedback": "string"
      }`,
      prompt: `
      Question: ${evalQuestion.question}
      Expected Topics: ${evalQuestion.expectedTopics.join(", ")}
      Answer: ${answer}
      Relevant FAQs Available: ${relevantFAQs.map((faq) => `"${faq.question}"`).join(", ")}
      
      Please evaluate this answer:`,
    })

    // Parse evaluation results
    let evaluation
    try {
      evaluation = JSON.parse(evaluationText)
    } catch (error) {
      console.error("Error parsing evaluation:", error)
      evaluation = {
        factualAccuracy: 0,
        relevance: 0,
        completeness: 0,
        staysWithinFAQScope: 0,
        overallScore: 0,
        feedback: "Error evaluating response",
      }
    }

    // Add to results
    results.push({
      question: evalQuestion.question,
      answer,
      relevantFAQs,
      evaluation,
    })
  }

  return results
}

// Function to generate Excel-compatible CSV data
export async function generateEvaluationCSV(): Promise<string> {
  const results = await evaluateAssistant()

  // Create CSV header
  let csv = "Question,Answer,Factual Accuracy,Relevance,Completeness,Stays Within FAQ Scope,Overall Score,Feedback\n"

  // Add each result as a row
  for (const result of results) {
    const { question, answer, evaluation } = result

    // Format cells and escape CSV special characters
    const formatCell = (text: string) => `"${text.replace(/"/g, '""')}"`

    csv +=
      [
        formatCell(question),
        formatCell(answer),
        evaluation.factualAccuracy,
        evaluation.relevance,
        evaluation.completeness,
        evaluation.staysWithinFAQScope,
        evaluation.overallScore,
        formatCell(evaluation.feedback),
      ].join(",") + "\n"
  }

  return csv
}
